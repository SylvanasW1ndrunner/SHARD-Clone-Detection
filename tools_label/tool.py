# evaluate_baselines.py (è·¯å¾„ç¡¬ç¼–ç ç‰ˆ)

import os
import numpy as np
import pandas as pd
import json
from sklearn.metrics import classification_report, recall_score
from tqdm import tqdm

# ==============================================================================
# 1. é…ç½®åŒºåŸŸ: è¯·åœ¨è¿™é‡Œä¿®æ”¹æ‚¨çš„æ–‡ä»¶è·¯å¾„
# ==============================================================================

# å®šä¹‰å­˜æ”¾æ‰€æœ‰åŸºçº¿å·¥å…·æ•°æ®çš„æ ¹ç›®å½•
BASELINE_DATA_DIR = "../../2_tools_label/dataset_II_contract_level/"

# è‡ªåŠ¨æ‹¼æ¥å„ä¸ªæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
TEST_CSV_PATH = "../test.csv"
CONTRACT_MAP_PATH = os.path.join(BASELINE_DATA_DIR, "index_map/contract_label_380.json")
OUTPUT_REPORT_PATH = "baseline_performance_report.txt"

# å®šä¹‰æ‰€æœ‰åŸºçº¿å·¥å…·åŠå…¶ç»“æœçŸ©é˜µçš„è·¯å¾„
# è„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨ BASELINE_DATA_DIR ä½œä¸ºå‰ç¼€
TOOLS = {
    "Deckard": "Deckard/compressed_matrix.npy",
    "EClone": "EClone/compressed_matrix.npy",
    "Nicad": "Nicad/compressed_matrix.npy",
    "SmartEmbed": "SmartEmbed/compressed_matrix.npy",
    "SourcererCC": "SourcererCC/compressed_matrix.npy",
}

# ==============================================================================
# 2. ä¸»è¯„ä¼°å‡½æ•°
# ==============================================================================

def explore_npy_file(filepath, max_rows=5):
    """
    æ¢ç´¢ .npy æ–‡ä»¶çš„å†…å®¹ï¼Œæ‰“å°åŸºæœ¬ä¿¡æ¯å’Œå‰å‡ è¡Œæ•°æ®

    å‚æ•°:
    filepath: .npy æ–‡ä»¶è·¯å¾„
    max_rows: æ˜¾ç¤ºçš„æœ€å¤§è¡Œæ•°ï¼Œé»˜è®¤ä¸º5
    """

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filepath):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return

    try:
        print(f"ğŸ“ æ­£åœ¨åŠ è½½æ–‡ä»¶: {filepath}")
        print("=" * 50)

        # åŠ è½½æ•°æ®
        data = np.load(filepath, allow_pickle=True)

        # åŸºæœ¬ä¿¡æ¯
        print("ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ•°æ®ç±»å‹: {type(data)}")
        print(f"   NumPy æ•°æ®ç±»å‹: {data.dtype}")
        print(f"   æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"   ç»´åº¦æ•°: {data.ndim}")
        print(f"   æ€»å…ƒç´ æ•°: {data.size}")
        print(f"   å†…å­˜å ç”¨: {data.nbytes / 1024:.2f} KB")
        print()

        # æ ¹æ®ç»´åº¦æ˜¾ç¤ºä¸åŒä¿¡æ¯
        if data.ndim == 0:
            # æ ‡é‡
            print("ğŸ“‹ æ•°æ®å†…å®¹ (æ ‡é‡):")
            print(f"   å€¼: {data}")

        elif data.ndim == 1:
            # ä¸€ç»´æ•°ç»„
            print("ğŸ“‹ æ•°æ®å†…å®¹ (ä¸€ç»´æ•°ç»„):")
            print(f"   é•¿åº¦: {len(data)}")
            print(f"   å‰ {min(max_rows, len(data))} ä¸ªå…ƒç´ :")
            for i, item in enumerate(data[:max_rows]):
                print(f"   [{i}]: {item}")
            if len(data) > max_rows:
                print(f"   ... (è¿˜æœ‰ {len(data) - max_rows} ä¸ªå…ƒç´ )")

        elif data.ndim == 2:
            # äºŒç»´æ•°ç»„ (ç±»ä¼¼è¡¨æ ¼)
            print("ğŸ“‹ æ•°æ®å†…å®¹ (äºŒç»´æ•°ç»„/è¡¨æ ¼):")
            print(f"   è¡Œæ•°: {data.shape[0]}")
            print(f"   åˆ—æ•°: {data.shape[1]}")

            # æ˜¾ç¤ºåˆ—ç´¢å¼•ä½œä¸º"è¡¨å¤´"
            print("\n   åˆ—ç´¢å¼•:", end="")
            for j in range(min(10, data.shape[1])):  # æœ€å¤šæ˜¾ç¤º10åˆ—
                print(f"{j:>10}", end="")
            if data.shape[1] > 10:
                print("       ...")
            else:
                print()

            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
            rows_to_show = min(max_rows, data.shape[0])
            print(f"\n   å‰ {rows_to_show} è¡Œæ•°æ®:")
            for i in range(rows_to_show):
                print(f"   [{i}]:", end="")
                cols_to_show = min(10, data.shape[1])  # æœ€å¤šæ˜¾ç¤º10åˆ—
                for j in range(cols_to_show):
                    # æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º
                    if np.issubdtype(data.dtype, np.floating):
                        print(f"{data[i, j]:>10.3f}", end="")
                    else:
                        print(f"{data[i, j]:>10}", end="")
                if data.shape[1] > 10:
                    print("       ...")
                else:
                    print()

            if data.shape[0] > max_rows:
                print(f"   ... (è¿˜æœ‰ {data.shape[0] - max_rows} è¡Œ)")

        else:
            # å¤šç»´æ•°ç»„
            print(f"ğŸ“‹ æ•°æ®å†…å®¹ ({data.ndim}ç»´æ•°ç»„):")
            print("   è¿™æ˜¯ä¸€ä¸ªé«˜ç»´æ•°ç»„ï¼Œæ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡ä¿¡æ¯:")

            # å°è¯•æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if np.issubdtype(data.dtype, np.number):
                print(f"   æœ€å°å€¼: {np.min(data)}")
                print(f"   æœ€å¤§å€¼: {np.max(data)}")
                print(f"   å¹³å‡å€¼: {np.mean(data):.3f}")
                print(f"   æ ‡å‡†å·®: {np.std(data):.3f}")

            # æ˜¾ç¤ºç¬¬ä¸€ä¸ª"åˆ‡ç‰‡"
            print(f"\n   ç¬¬ä¸€ä¸ªåˆ‡ç‰‡ [0] çš„å½¢çŠ¶: {data[0].shape}")
            print(f"   ç¬¬ä¸€ä¸ªåˆ‡ç‰‡çš„å‰å‡ ä¸ªå…ƒç´ :")
            flat_slice = data[0].flatten()
            for i, item in enumerate(flat_slice[:10]):
                print(f"   [{i}]: {item}")
            if len(flat_slice) > 10:
                print(f"   ... (è¯¥åˆ‡ç‰‡è¿˜æœ‰ {len(flat_slice) - 10} ä¸ªå…ƒç´ )")

        # ç‰¹æ®Šæ•°æ®ç±»å‹å¤„ç†
        if data.dtype == 'object':
            print("\nâš ï¸  æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªå¯¹è±¡æ•°ç»„ï¼Œå¯èƒ½åŒ…å«å¤æ‚çš„Pythonå¯¹è±¡")
            print("   ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹:", type(data.flat[0]) if data.size > 0 else "ç©ºæ•°ç»„")

        print("\n" + "=" * 50)
        print("âœ… æ–‡ä»¶æ¢ç´¢å®Œæˆ!")

        return data

    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def evaluate_all_baselines():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºè¯„ä¼°æ‰€æœ‰åŸºçº¿å·¥å…·çš„æ€§èƒ½ã€‚
    """
    # --- åŠ è½½å¿…è¦çš„æ–‡ä»¶ ---
    print("æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®å’Œæ˜ å°„æ–‡ä»¶...")
    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        with open(CONTRACT_MAP_PATH, 'r', encoding='utf-8') as f:
            name_to_idx = json.load(f)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°å¿…è¦æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥é…ç½®åŒºåŸŸçš„è·¯å¾„ - {e}")
        return

    # æ‰“å¼€æŠ¥å‘Šæ–‡ä»¶å‡†å¤‡å†™å…¥
    with open(OUTPUT_REPORT_PATH, "w", encoding="utf-8") as report_file:
        report_file.write("="*30 + " åŸºçº¿å·¥å…·æ€§èƒ½è¯„ä¼°æŠ¥å‘Š " + "="*30 + "\n\n")
        print(f"æŠ¥å‘Šå°†ä¿å­˜è‡³: {OUTPUT_REPORT_PATH}")

        # --- éå†æ¯ä¸€ä¸ªå·¥å…·è¿›è¡Œè¯„ä¼° ---
        for tool_name, matrix_suffix in TOOLS.items():
            matrix_path = os.path.join(BASELINE_DATA_DIR, matrix_suffix)

            print(f"\n--- æ­£åœ¨è¯„ä¼°å·¥å…·: {tool_name} ---")
            report_file.write(f"--- å·¥å…·: {tool_name} ---\n")

            if not os.path.exists(matrix_path):
                print(f"  - è­¦å‘Š: æ‰¾ä¸åˆ°ç»“æœçŸ©é˜µ {matrix_path}ï¼Œè·³è¿‡è¯¥å·¥å…·ã€‚")
                report_file.write("  - ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è¯„ä¼°ã€‚\n\n")
                continue
            with open(matrix_path,'rb') as f:
                result_matrix = np.unpackbits(np.load(f))[:144400].reshape((380, 380))
                print(f"  - è¯»å–ç»“æœçŸ©é˜µ {matrix_path} æˆåŠŸã€‚")
                print(f"  - çŸ©é˜µå¤§å°ï¼š", result_matrix.ndim)
            y_true = []
            y_pred = []

            # ä½¿ç”¨ .to_dict('records') å¯ä»¥æ›´é«˜æ•ˆåœ°è¿­ä»£DataFrame
            for row in tqdm(test_df.to_dict('records'), desc=f"Processing {tool_name}"):
                id1, id2 = row['contract_id'], row['clone_contract_id']
                groundtruth = int(row['groundtruth'])

                idx1 = name_to_idx.get(id1)
                idx2 = name_to_idx.get(id2)

                if idx1 is None or idx2 is None:
                    continue

                predicted_type = result_matrix[idx1, idx2]
                pred_binary = 1 if predicted_type > 0 else 0

                y_true.append(groundtruth)
                y_pred.append(pred_binary)

            if not y_true:
                print("  - é”™è¯¯: æœªèƒ½å¤„ç†ä»»ä½•æœ‰æ•ˆçš„æ•°æ®å¯¹ã€‚è¯·æ£€æŸ¥CSVå’ŒJSONæ–‡ä»¶çš„åˆçº¦åç§°æ˜¯å¦åŒ¹é…ã€‚")
                report_file.write("  - æœªèƒ½å¤„ç†ä»»ä½•æœ‰æ•ˆçš„æ•°æ®å¯¹ã€‚\n\n")
                continue

            # --- è®¡ç®—å¹¶ä¿å­˜æ•´ä½“æŒ‡æ ‡ ---
            report_str = classification_report(y_true, y_pred, target_names=["Not Clone (Type 0)", "Clone (Type 1-4)"], digits=4)
            print("\n  --- æ•´ä½“æ€§èƒ½æŒ‡æ ‡ ---")
            print(report_str)
            report_file.write("\næ•´ä½“æ€§èƒ½æŒ‡æ ‡:\n")
            report_file.write(report_str + "\n")

            # --- è®¡ç®—å¹¶ä¿å­˜å„ç±»å‹ç‹¬ç«‹çš„å¬å›ç‡ ---
            test_df['predicted'] = y_pred
            clone_df = test_df[test_df['groundtruth'] == 1]

            print("\n  --- å„ç±»å‹å…‹éš†çš„ç‹¬ç«‹å¬å›ç‡ (Recall) ---")
            report_file.write("\nå„ç±»å‹å…‹éš†çš„ç‹¬ç«‹å¬å›ç‡ (Recall):\n")

            for clone_type in [1.0, 2.0, 3.0, 4.0]: # ç›´æ¥è¿­ä»£æ‰€æœ‰å¯èƒ½çš„ç±»å‹
                type_df = clone_df[clone_df['type'] == clone_type]
                if len(type_df) == 0:
                    recall_str = f"  - Type-{int(clone_type)} Recall: N/A (æµ‹è¯•é›†ä¸­æ— æ­¤ç±»å‹æ ·æœ¬)\n"
                else:
                    recall = recall_score(type_df['groundtruth'], type_df['predicted'], zero_division=0)
                    correctly_found = type_df['predicted'].sum()
                    total = len(type_df)
                    recall_str = f"  - Type-{int(clone_type)} Recall: {recall:.4f} ({correctly_found} / {total})\n"

                print(recall_str.strip())
                report_file.write(recall_str)

            report_file.write("\n" + "="*70 + "\n\n")
            print(f"--- {tool_name} è¯„ä¼°å®Œæˆ ---")

    print("\næ‰€æœ‰åŸºçº¿å·¥å…·è¯„ä¼°å®Œæ¯•ï¼")


# ==============================================================================
# 3. è„šæœ¬ä¸»å…¥å£
# ==============================================================================

if __name__ == '__main__':
    # ç›´æ¥è°ƒç”¨ä¸»å‡½æ•°ï¼Œä¸å†éœ€è¦argparse
    evaluate_all_baselines()